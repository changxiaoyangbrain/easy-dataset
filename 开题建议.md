# NuCorpus 核应急领域大模型数据集构建系统——开题建议

> 说明：本文档面向本科毕业设计“开题”环节，依据截图中的提示逐条作答，并结合 NuCorpus（本仓库）现状与 `设计说明0104.md` 的技术说明给出可执行的研究方案与评测设计。本文不构成任何实际核设施应急处置指南，真实场景必须以国家/IAEA/营运单位的权威预案与规程为准。

## 1. 针对截图问题的逐条回答

### 1.1 拿到了什么资料（语料与参考规范）
建议在开题报告中将“已具备资料”分为三类，并给出可追溯链接（含版权/公开性说明）：

1) **权威法规/标准/预案（公开）**
- 《国家核应急预案》（2013 修订）：https://www.gov.cn/gongbao/content/2013/content_2449468.htm
- IAEA GSR Part 7《Preparedness and Response for a Nuclear or Radiological Emergency》（核或辐射应急准备与响应）：https://www.iaea.org/publications/10905/preparedness-and-response-for-a-nuclear-or-radiological-emergency
- IAEA Safety Glossary（核安全术语表）：https://www.iaea.org/resources/safety-standards/safety-glossary
- 《中华人民共和国核安全法》：https://www.mee.gov.cn/ywgz/fgbz/fl/202110/t20211028_958223.shtml
- 《核电厂核事故应急管理条例》：https://www.mee.gov.cn/ywgz/fgbz/xzfg/201906/t20190628_707989.shtml

2) **领域知识梳理文档（项目内已形成）**
- `docs/核应急领域内容与范畴.md`：核应急领域范围、理论框架、流程环节与关键术语。
- `docs/核应急领域数据集分类与分析.md`：数据集模块划分与构建建议。
- `docs/8个数据集原始文档搜集.md`：公开语料搜集路径（白皮书/事故报告/科普/医学等）。

3) **工程实现与评测材料（项目内已有能力）**
- 多格式文档处理与 PDF 多策略解析（default/vision/mineru/mineru-local），见 `docs/tech-analysis-docling-deepseek-ocr.md`。
- 核应急专用 Prompt（规程化/判别题）与数据集质量评估 Prompt，见 `lib/llm/prompts/nuclear.js`、`lib/llm/prompts/datasetEvaluation.js`。

### 1.2 要解决什么问题（研究问题定义）
建议将问题明确为“通用大模型在核应急场景的不可控与不可审计”，并落到可研究的工程目标：

- **问题 P1：多源文档难以规模化转为高质量训练样本**：法规/预案/报告格式各异，PDF 扫描件、表格、目录层级导致自动切分与对齐困难。
- **问题 P2：核应急问答对的质量要求极高**：术语、流程、条件触发与安全文化约束严苛，通用生成容易出现不一致或“编造”。
- **问题 P3：缺少面向核应急的可复用数据生产流水线**：从数据采集→切分→生成→审核→导出/发布缺少一体化工具。
- **问题 P4：缺少可操作的评测与验收标准**：需要建立“准确性/安全性/规范性/可追溯”等多维指标并可复现评测。

### 1.3 现有大模型有什么不足（并给出简单应急问题示例）
**不足（开题写法建议）**
- **领域知识缺失**：对 IAEA 术语、国内法规结构、核应急分级/组织体系掌握不足。
- **幻觉与不确定表达**：容易在关键事实（阈值、责任主体、流程先后）上“猜测”，难以审计。
- **长文档与结构化信息理解不足**：对条款层级、表格、附件等结构化信息提取不稳定。
- **安全合规风险**：可能给出不适当、夸大或不负责任的建议；在高风险场景中不可直接信任。

**可在开题中展示的“简单核应急响应问题”（用于说明通用模型短板，不涉及具体操作阈值）**
- Q1：核应急管理的全周期包含哪些阶段？各阶段的主要目标是什么？
- Q2：我国核设施应急状态一般如何分级？分级的管理意义是什么？
- Q3：EAL（应急行动水平）与 OIL（操作干预水平）分别用于什么决策？
- Q4：公众防护行动通常包括哪些类型（如隐蔽、疏散、稳定性碘等）？适用条件如何概括？
- Q5：核应急信息发布应遵循哪些基本原则（权威、及时、避免恐慌等）？
- Q6：核事故医学救援的“去污—分诊—治疗”一般逻辑是什么？

### 1.4 目标达到什么效果（预期成果）
建议将“效果”拆成系统产物 + 数据产物 + 评测产物：

- **系统产物**：完成一套可运行的数据集构建系统（NuCorpus），支持上传→解析→切分→生成→审核→导出/发布。
- **数据产物**：产出“核应急领域专项 SFT 数据集（公开部分）”，覆盖理论/法规/技术/管理/案例/科普等模块。
- **评测产物**：形成一套可复现评测集与指标体系，能对“基座模型 vs 领域适配（微调/RAG）”进行对比。

### 1.5 方法的构建（如何把目标落地）
结合本项目现状，建议在开题中采用“工程流水线 + 人在回路 + 可评测”的方法论：

1) **语料获取与分层**：按“权威规范/技术材料/科普材料”分层，建立元数据（来源、公开性、版本）。
2) **结构化切分**：优先利用标题层级切分（Markdown heading split），对长段再做递归/字符/token 分块（LangChain splitters）。
3) **提示词工程**：分别为“问题生成/答案生成/质量评估/去噪与 CoT 优化”设计 Prompt；核应急场景使用专用 Prompt（EOP/JUDGEMENT）。
4) **质量闭环**：AI 预评估 + 人工抽检/复核；不通过样本返工（重切分/重生成/人工编辑）。
5) **导出与发布**：导出 Alpaca、ShareGPT、multilingual-thinking 等格式；支持上传 HuggingFace（便于公开复现）。
6) **可选：结构化知识增强**：对文本块进行三元组抽取，入库 Neo4j 形成图谱，用于后续 RAG/多跳推理验证。

### 1.6 选用的工具（与本项目一致）
开题中建议把“工具”按用途写清楚（并说明为什么适合核应急语料）：

- **前端/交互**：Next.js 14 + React 18 + MUI v5（适合快速搭建审核与编辑 UI）。
- **后端与任务**：Node.js + Next.js API Routes（便于将处理能力做成可复用接口）。
- **数据层**：Prisma ORM + SQLite（本地可复现、轻量部署，适合毕业课题迭代）。
- **切分与文本处理**：@langchain/textsplitters（多策略切分，适合长文档）。
- **PDF 解析**：default/vision/mineru/mineru-local 四策略（兼顾离线、成本与高精度）。
- **数据集导出/发布**：JSON/JSONL/CSV 导出 + HuggingFace Hub 上传（见 `app/api/projects/[projectId]/huggingface/upload/route.js`）。
- **可选图谱**：Neo4j（适用于法规条款、事故征兆、操作策略的关系建模）。

### 1.7 技术路线（微调 + 知识库/RAG 的组合）
建议在开题中给出“双路线”并说明对比目的（不是二选一）：

```
语料（法规/预案/报告/科普）
  ↓ 解析(PDF/DOCX/MD) + 清洗
  ↓ 切分(标题层级 + 递归分块)
  ↓ QA 生成(Prompt) + 质量评估 + 人审
  ↓ 数据集导出(Alpaca/ShareGPT/...)
  ↓ 微调(LoRA/QLoRA) → 领域模型

并行路线（验证/应用）：
切分后的 chunks → 向量化/检索（知识库RAG）→ 领域问答（与微调模型对比）
```

开题建议写清楚：
- **微调（SFT）**：解决“表达风格/术语/流程习惯”的内化问题。
- **RAG（知识库）**：解决“事实追溯/版本更新/减少幻觉”的外部约束问题。
- **对比评测**：用统一评测集验证两者在核应急任务上的收益与边界。

### 1.8 验证检验：指标维度与专家打分
建议至少定义 4 个“核应急友好”的评测维度，并给出权重与评分规则（1–5 分），同时保留“证据追溯”字段便于审计：

1) **技术准确性（建议权重 40%）**：事实、概念、流程顺序正确；不出现与权威文本冲突的结论。  
2) **安全与合规（建议权重 30%）**：不鼓励违规操作、不制造恐慌；符合“保守决策、纵深防御”的安全文化。  
3) **术语与表述规范（建议权重 20%）**：术语一致、定义严谨、避免口语化歧义。  
4) **可追溯与可解释（建议权重 10%）**：答案能指向对应条款/段落/章节（在数据标注中保留 chunkId/来源）。  

**专家打分组织方式（开题可写）**
- 评审对象：抽取 N 条样本（如 200 条），按模块均匀覆盖。
- 评审人：核工程/应急管理/辐射防护方向教师或行业专家（≥2 人）。
- 一致性：计算评审一致性（如 Cohen’s Kappa 或简单一致率），用于说明评测可靠性。
- 工程化：项目内已提供“数据集质量评估 Prompt”作为 AI 预评估参考（`lib/llm/prompts/datasetEvaluation.js`），但最终以专家结论为准。

### 1.9 参考其他垂直领域大模型的构建方法（可写作“相关工作”）
建议将“垂直领域模型”归纳为三条通用路径，并给出典型代表：

- **领域继续预训练（DAPT）**：用领域语料继续预训练（如 SciBERT、BioBERT），提升术语与语法分布适配。
- **指令微调（SFT/Alignment）**：用领域 QA/指令数据对齐输出格式与任务能力（Self-Instruct、Alpaca 等思路）。
- **RAG/知识库增强**：将权威文档接入检索系统，降低幻觉并支持版本更新（RAG 框架）。

核应急建议优先采用：**SFT（风格与流程） + RAG（权威可追溯）** 的组合，并用统一评测集对比收益。

### 1.10 学习核应急相关知识（流程、规范）建议
为避免“只做工程不懂业务”，开题中建议明确学习清单：

- **应急全周期**：准备（Preparedness）→ 响应（Response）→ 恢复（Recovery），以及组织体系与职责分工。
- **关键概念**：EAL/OIL、应急计划区、公众防护行动、辐射监测与后果评价、医学救援与去污。
- **权威来源**：IAEA GSR Part 7、IAEA Safety Glossary、我国《国家核应急预案》等。
- **落地方式**：将术语表/分级体系/流程图转为结构化标签树与问答模板，指导数据生成与审核。

### 1.11 数据缩小到核应急环境里（范围收敛策略）
建议开题明确“边界”，避免题目过大：

- **范围边界**：仅覆盖公开资料；只做“核或辐射应急”知识与管理流程，不涉及敏感设施细节与内部操作阈值。
- **模块化收敛**：按 `docs/核应急领域数据集分类与分析.md` 的七大模块组织数据，优先做“理论/法规/技术支持系统/公众防护”四类。
- **数据治理**：保留元数据（来源、年份、版本、模块、语言）；去重、去噪、统一术语（同义词表）。

## 2. 基于 NuCorpus 的研究内容与创新点（建议写进开题）
可按“工程创新 + 研究创新”表述：

- **创新点 I（工程）**：面向核应急的“一站式数据集生产流水线”，将解析/切分/生成/审核/导出整合到同一系统，显著降低数据生产门槛。
- **创新点 II（方法）**：核应急专用 Prompt 体系（规程化/判别题/质量评估），强调安全文化与术语规范，减少幻觉。
- **创新点 III（评测）**：建立核应急问答的多维指标与专家评审机制，形成可复现的验收方法。
- **创新点 IV（可选）**：引入知识图谱抽取（实体-关系-实体）增强结构化知识管理与检索验证能力。

## 3. 预期里程碑（可直接用于开题“进度计划”）
可按 8–10 周/学期节奏写（示例，可按学校日程调整）：

1. 第 1–2 周：明确范围与资料清单；搭建运行环境；完成语料公开性审查。
2. 第 3–4 周：完成 PDF/多格式解析与切分策略定型；建立标签体系与数据模块。
3. 第 5–6 周：完成 Prompt 设计与问答生成；建立人审规范与抽检流程。
4. 第 7–8 周：导出训练数据；完成至少一轮微调与/或 RAG 基线（对比实验）。
5. 第 9–10 周：完成评测（专家打分 + 自动统计）；撰写论文与整理可复现材料。

## 4. 风险与对策（开题常见加分项）
- **语料版权/敏感性风险**：仅使用公开资料；对来源与许可做记录；不纳入涉敏阈值与内部规程细节。
- **模型幻觉风险**：采用 RAG 约束与“缺失即回答未知”的策略；引入专家抽检闭环。
- **评测主观性风险**：制定评分细则与示例；至少双人评审并计算一致性。
- **工程周期风险**：先保证“数据流水线 + 可评测”最小闭环，再做图谱/更多模型扩展。

## 参考文献与链接（建议开题报告引用）
[1] Vaswani A, Shazeer N, Parmar N, et al. Attention Is All You Need（中文名：注意力机制即你所需）. 2017. https://arxiv.org/abs/1706.03762  
[2] Brown T B, Mann B, Ryder N, et al. Language Models are Few-Shot Learners（中文名：语言模型是少样本学习者）. 2020. https://arxiv.org/abs/2005.14165  
[3] Wei J, Wang X, Schuurmans D, et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models（中文名：思维链提示促进大语言模型推理）. 2022. https://arxiv.org/abs/2201.11903  
[4] Lewis P, Perez E, Piktus A, et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks（中文名：检索增强生成用于知识密集型任务）. 2020. https://arxiv.org/abs/2005.11401  
[5] Hu E J, Shen Y, Wallis P, et al. LoRA: Low-Rank Adaptation of Large Language Models（中文名：LoRA：大语言模型的低秩适配）. 2021. https://arxiv.org/abs/2106.09685  
[6] Dettmers T, Pagnoni A, Holtzman A, et al. QLoRA: Efficient Finetuning of Quantized LLMs（中文名：QLoRA：量化大模型的高效微调）. 2023. https://arxiv.org/abs/2305.14314  
[7] Lee J, Yoon W, Kim S, et al. BioBERT（中文名：BioBERT：面向生物医学文本的预训练模型）. 2019. https://arxiv.org/abs/1901.08746  
[8] Beltagy I, Lo K, Cohan A. SciBERT（中文名：SciBERT：科学文本预训练语言模型）. 2019. https://arxiv.org/abs/1903.10676  
[9] Chalkidis I, Fergadiotis M, Malakasiotis P, et al. LEGAL-BERT（中文名：LEGAL-BERT：法律领域预训练模型）. 2020. https://arxiv.org/abs/2010.02559  
[10] IAEA. Preparedness and Response for a Nuclear or Radiological Emergency (GSR Part 7)（中文名：核或辐射应急准备与响应）. https://www.iaea.org/publications/10905/preparedness-and-response-for-a-nuclear-or-radiological-emergency
