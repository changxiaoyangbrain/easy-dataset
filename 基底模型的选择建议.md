# 基底模型的选择建议（核应急领域数据集微调 / 本地封闭部署）

> 版本：2026-01-04  
> 适用对象：已使用 NuCorpus 产出核应急领域 SFT 数据集，希望选择“可本地封闭运行、中文能力强、可控性高”的开源基底模型进行微调与部署。  
> 说明：核应急属于安全关键领域，微调不等于“可直接用于真实应急处置”。建议将微调与权威文档检索（RAG）、人工复核与审计机制结合使用。

## 1. 结论（先给可落地的推荐）

### 1.1 默认首选（综合最优：中文 + 长上下文 + 许可证友好 + 易本地化）
- **Qwen3-30B-A3B-Instruct-2507（非思考模式，Apache-2.0）**  
  - 推荐用作核应急数据集的主力基底模型：中文强、长上下文（原生 262,144 tokens）、输出稳定（不生成 `<think>` 块，利于结构化/规范化回答）。  
  - 模型链接：https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507

### 1.2 追求最佳效果（算力充足）
- **Qwen3-Next-80B-A3B-Instruct（非思考模式，Apache-2.0）**  
  - 适合追求更强推理与文本生成质量的封闭部署场景；同样支持超长上下文（原生 262,144，可扩展到约 1,010,000）。  
  - 模型链接：https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct

### 1.3 资源受限 / 快速迭代（单卡或轻量服务器）
- **Qwen3-4B-Instruct-2507（非思考模式，Apache-2.0）**  
  - 适合验证数据集质量、快速迭代提示词与训练流程；也可作为端侧/边缘封闭部署的候选。  
  - 模型链接：https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507

### 1.4 更偏“推理型”的备选（希望模型更擅长多步推断）
- **DeepSeek-R1-Distill-Qwen-32B（MIT）**  
  - 推理风格更强，适合“事故演化逻辑/因果链/决策依据”类任务；上下文上限 131,072。  
  - 模型链接：https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B

## 2. 为什么核应急选型更“挑基底模型”

结合 NuCorpus 的数据生产方式（上传→切分→问答生成→导出），以及核应急领域特点，基底模型需要同时满足：

1. **中文与专业术语能力**：法规条文/导则/预案的中文表达严谨，术语（EAL/OIL、应急分级、辐射监测等）要求一致。  
2. **长上下文与结构理解**：核应急资料常为长篇法规、手册、报告；长上下文有助于 RAG 拼接证据或直接输入长段落。  
3. **本地封闭部署可行性**：模型权重可离线保存，推理可在内网/断网环境运行；许可证对“本地私有使用/研究”友好。  
4. **输出可控与可审计**：能遵循“缺失即回答未知”、不臆造阈值与流程；最好支持稳定的结构化输出（JSON、条目化步骤）。  

## 3. NuCorpus 对模型的兼容性要求（与工程实现对齐）

NuCorpus 支持多种 Provider（见 `lib/llm/core/index.js`），在“封闭部署”下通常有两种接入方式：

1) **Ollama（本地一体化）**  
- 优点：部署简单、适合离线；大量模型提供 GGUF 量化版本。  
- 注意：超长上下文会显著增加 KV Cache 占用；需要根据硬件设定合理的上下文长度上限。

2) **OpenAI-compatible 本地推理服务（推荐用于生产）**  
- 用 vLLM / SGLang / TGI 等在内网启动推理服务，NuCorpus 按 OpenAI 接口方式调用（provider 走 `openai` 或兼容项）。  
- 优点：吞吐高、工程化强、对长上下文支持更完整；便于做访问控制与审计。

另外，NuCorpus 内置了对 `<think>` / `reasoning_content` 的提取逻辑（见 `LLMClient.extractAnswerAndCOT()`），所以“思考型模型”也能接入；但在需要严格 JSON 输出的步骤中，建议优先选用“非思考模式”模型以减少解析不确定性。

## 4. 2026-01 候选基底模型速览（来自 Hugging Face 模型卡/配置）

| 模型 | 许可证 | 上下文上限（来自 config / 模型卡） | 特点 | 适配结论 |
|---|---|---:|---|---|
| Qwen/Qwen3-30B-A3B-Instruct-2507 | Apache-2.0 | 262,144（原生） | 中文强、非思考模式、长上下文 | **默认首选** |
| Qwen/Qwen3-Next-80B-A3B-Instruct | Apache-2.0 | 262,144（原生，可扩展更长） | 更强能力、更高资源需求 | **高配首选** |
| Qwen/Qwen3-4B-Instruct-2507 | Apache-2.0 | 262,144（原生） | 轻量、输出稳定、适合迭代 | **低配首选** |
| deepseek-ai/DeepSeek-V3.2 | MIT | 163,840（`max_position_embeddings`） | 偏推理/Agent，模型较重 | 备选（高配） |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-32B | MIT | 131,072（`max_position_embeddings`） | 推理风格强、相对可训练 | 推理型备选 |
| internlm/internlm3-8b-instruct | Apache-2.0 | 32,768（`max_position_embeddings`） | 中文可用、生态相对成熟 | 低配备选 |
| openbmb/MiniCPM3-4B | Apache-2.0 | 32,768（`max_position_embeddings`） | 小模型、便于端侧 | 端侧备选 |
| meta-llama/Llama-4-* Instruct | 其他（受限许可） | 需授权后获取 | 英文/多语强，下载常需授权 | 封闭环境需额外合规评估 |

模型链接（便于核验）：  
- https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507  
- https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct  
- https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507  
- https://huggingface.co/deepseek-ai/DeepSeek-V3.2  
- https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B  
- https://huggingface.co/internlm/internlm3-8b-instruct  
- https://huggingface.co/openbmb/MiniCPM3-4B  

## 5. 按“硬件/目标”给出更具体的选择建议

### 5.1 如果你只能用单机单卡（典型 24GB 显存级别）
- **训练**：优先从 `Qwen3-4B-Instruct-2507` 开始做 LoRA/QLoRA，先把数据集质量跑通（数值/术语/流程一致性）。  
- **部署**：4-bit 量化后可用 Ollama / vLLM 部署在本地；通过 NuCorpus 的 `ollama` 或 OpenAI-compatible endpoint 接入。

### 5.2 如果你有 48–80GB 显存或多卡服务器（更接近“最终可用”）
- **训练/部署主力**：`Qwen3-30B-A3B-Instruct-2507`。  
- **理由**：核应急更看重“中文严谨表达 + 长文档理解 + 输出可控”；该模型提供原生 256K 级别上下文，并且非思考模式更利于生产稳定性。

### 5.3 如果你追求最高效果且算力充足
- **训练/部署**：`Qwen3-Next-80B-A3B-Instruct`。  
- **注意**：大上下文会带来 KV Cache 的巨大显存占用；实际生产建议结合 NuCorpus 的切分与 RAG，将单次输入控制在合理范围（如 8K–32K）。

### 5.4 如果你的任务“强推理优先”（事故演化、因果链、方案对比）
- **备选基底**：`DeepSeek-R1-Distill-Qwen-32B`（或同系列更小蒸馏版）。  
- **建议**：将 NuCorpus 生成的 CoT（思维链）作为可选字段训练；并保留“缺失即回答未知”的样本，抑制模型在阈值与规程步骤上的编造倾向。

## 6. 微调与封闭部署的工程化建议（与 NuCorpus 输出对齐）

1) **基底模型建议选 Instruct 版**：NuCorpus 导出的 Alpaca/ShareGPT 本质是指令微调数据，使用 Instruct 基座通常更省训练成本。  
2) **优先 QLoRA / LoRA**：核应急数据集更需要多轮迭代与人工抽检；参数高效微调更利于快速试验与回滚。  
3) **部署时加入“检索增强 + 审计”**：对核应急问答，推荐将权威文档（预案/法规）放入本地知识库，生成时要求给出依据片段或引用段落编号，并记录查询日志。  
4) **把 NuCorpus 当作数据工厂**：数据生产阶段可以用更强模型（若合规且不出域），最终落地模型用可离线部署的开源基底完成训练与封闭部署。

## 7. 你可以直接采用的“最终建议”

- **首选基底模型**：`Qwen/Qwen3-30B-A3B-Instruct-2507`  
- **低配替代**：`Qwen/Qwen3-4B-Instruct-2507`（先跑通流程，再升级）  
- **高配替代**：`Qwen/Qwen3-Next-80B-A3B-Instruct`  
- **推理型备选**：`deepseek-ai/DeepSeek-R1-Distill-Qwen-32B`

